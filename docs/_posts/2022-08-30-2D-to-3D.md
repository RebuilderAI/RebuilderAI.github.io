---
date: 2022-08-30 06:00:00  
layout: post  
title: 우리는 왜 2D에서 3D로 이동하는가
subtitle: 2D와 3D의 장점과 단점, 그리고 우리의 방향성
description: Difference of 2D and 3D
image:  https://user-images.githubusercontent.com/36464983/187398783-4ad9b214-f9fd-4659-acfe-363c58c05add.png
category: 2D, 3D
tags:
  - Image salient object detecion
  - 3D vision
  - Point cloud segmentation
author: ansh941
---

# 우리는 왜 2D에서 3D로 이동하는가

2D와 3D의 장점과 단점, 그리고 우리의 방향성

# 요약

저희는 2D에서의 Salient Object Detection(SOD) task를 연구하면서 2D 공간에서의 한계를 느끼고 3D로 넘어가는 단계에 있으며, 이전의 연구 과정에서 느낀 2D에서의 한계는 다음과 같습니다.

1. 프레임의 흐름에 따른 일관성을 보장할 수 없다.
2. 확대된 경우 제대로 된 mask를 추정할 수 없다.

이러한 두 가지 한계점은 저희의 서비스에 큰 걸림돌이 되기 때문에, 이 문제들은 반드시 해결해야 합니다. 많은 고민의 끝에 저희는 3D의 knowledge를 사용한다면 이러한 문제들을 완화 또는 해결할 수 있을 것이라 생각했습니다. 하지만 3D 공간에서의 작업은 그들만의 단점이 존재했습니다.

따라서 저희는 2D와 3D를 융합해 각각 도메인이 서로의 단점을 보완하는 방향으로 연구한다면 이러한 문제를 해결할 수 있을 것이라 생각했고, 현재 개발의 진행 중에 있습니다.

# 2D의 장점

## 1. 이미지만 있으면 된다.

### 2D SOD 모델의 간략한 구조

![architecture1](https://user-images.githubusercontent.com/36464983/187398992-fdcf0980-f58d-4128-978e-7be45f2ebe60.png)

현재 알고리즘의 경우 이와 같은 형태의 알고리즘으로, input의 이미지만 있으면 detection된 결과를 볼 수 있습니다.

## 2. 관련 연구가 많으며 성능이 높다.

Image 관련의 SOD 연구들은 이전부터 현재까지도 활발히 연구되고 있는 분야로, 데이터만 잘 준비되어 있다면 높은 성능의 결과를 볼 수 있습니다.

![Untitled](https://user-images.githubusercontent.com/36464983/187399125-9a91dfb9-881b-494a-a4a9-998abbf74d19.png)

# 2D의 단점

## 1. 확대된 물체를 탐지할 수 없으며, 프레임의 흐름에 따른 일관성이 떨어진다.

### 2D SOD의 결과

![IMG_5085.jpeg](https://user-images.githubusercontent.com/36464983/187399186-b2725681-1b84-470e-8c8c-34978bd8722c.jpeg)

![00066.jpg](https://user-images.githubusercontent.com/36464983/187399243-df2d03a4-bc7b-4b18-bf76-8987eac94fce.jpg)

저희가 현재 적용 중인 2D 데이터에서의 SOD 결과입니다. 기존 SOD 모델의 경우 한 이미지 내에서 가장 돋보이는 영역을 탐지하는 것으로, 이처럼 확대하게 되면 그 안에서 가장 돋보이는 영역 또는 물체를 탐지할 수 밖에 없어 위와 같은 결과를 보이게 됩니다.

저희는 비디오가 연속적으로 흘러가면서 물체의 위치가 변하더라도 일관성 있게 그 영역을 탐지하기를 원합니다. 하지만 위 결과를 보면 현재는 그럴 수 없다는 것을 알 수 있습니다. 이것은 알고리즘의 한계로, 기존 모델에 후보정 네트워크를 붙여 이러한 한계점을 보강했지만 완벽치 않다는 것을 아실 수 있습니다. 이 경우 이전의 마스크를 참조하는 것으로 변화가 클수록 성능은 더욱 안 좋을 것입니다.

# 3D의 장점

## 1. 물체가 확대되거나 이동하더라도 일관성 있게 detection이 가능하다.
![Untitled](https://user-images.githubusercontent.com/36464983/187399359-1dddfe72-9586-4200-ba90-4c6507d935da.png)

일반적으로 point cloud 복원을 위해 영상을 찍으면서 디테일을 살리고자 가까이도 가고 하는데, 그 모든 정보를 받아들여 point cloud를 만들고, 그렇게 구축된 3D point cloud에서의 segmentation을 진행하기 때문에 확대된 프레임의 camera 위치에서 detection 하더라도 일관성이 보장됩니다.

![snapshot01.png](https://user-images.githubusercontent.com/36464983/187399483-a2ae182d-7d46-48ca-9865-b2ae01c7b524.png)

위 결과가 영상을 기반으로 만든 point cloud를 segmentation한 것입니다.

# 3D의 단점

## 1. Point cloud가 제대로 나올 수 있어야 한다.

이 task를 수행하기 위한 전제 조건이라고도 할 수 있는 문제입니다. task를 진행하기에 앞서 point cloud가 구축이 되어야 3D에서의 segmentation을 수행할 수 있습니다. 하지만 feature가 적거나 없는 경우, point cloud 자체가 제대로 형성이 안 되어 탐지에 문제가 될 수 있습니다.

## 2. 데이터의 크기가 크다.

데이터의 크기가 매우 커 동시 처리 개수가 매우 적어질 수 있습니다. 그렇기 때문에 보통 대표적인 point를 추출하고 그에 대해 연산을 하게 되는데, 이 경우 정보를 잃게 되어 성능이 떨어지는 결과를 불러오게 됩니다.

# 우리의 방향성

저희는 이 둘의 장점을 모두 활용하면서 단점을 최소화 하기 위해 두 도메인 모두를 활용하고자 합니다.

참고할 수 있는 아키텍처 및 이전의 논문으로는 BPNet이 있으며, 그림으로는 다음과 같이 나타낼 수 있습니다.
![Untitled](https://user-images.githubusercontent.com/36464983/187399562-55b430df-9cf4-495e-97eb-52ebd83a00a4.png)

![Untitled](https://user-images.githubusercontent.com/36464983/187399647-f70ea319-c802-4bea-a52a-125ebb8d29df.png)

이와 같은 기법을 적용했을 때, 2D 혹은 3D만을 사용한 것에 비해 성능이 향상된 결과를 보였으며, 모델을 개발하고 서비스에 적용한다면 더욱 디테일하고 깔끔한 모델을 보일 수 있을 것이라 예상됩니다.

# Reference

1. [paperswithcode.com](http://paperswithcode.com)
2. **PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation** [https://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf)
3. **Bidirectional Projection Network for Cross Dimension Scene Understanding**
[https://wbhu.github.io/projects/BPNet/](https://wbhu.github.io/projects/BPNet/)
