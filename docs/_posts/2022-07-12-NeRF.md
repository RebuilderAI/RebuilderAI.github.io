---
date: 2022-07-12
layout: post
title: 'Neural Radiance Fields'
subtitle: Tech blog
description: ECCV 2020 NeRF
image: https://user-images.githubusercontent.com/55485826/178452238-a4eb2b1b-07f2-49d5-ab59-1dfa829be00e.png
category: Reconstruction, Novel View Synthesis
tags:
  - vision
  - 3d reconstruction
  - graphics
use_math: true
author: jgnooo
---

# Neural Radiance Fields
---

오늘 RebuilderAI AI팀에서 소개할 기술은 요즘 Novel view synthesis 분야에서 많은 인기를 끌고 있는 Neural Radiance Fields(NeRF) 입니다.

Novel view synthesis란 새로운 시점(viewpoint)에서 보여지는 이미지를 rendering하는 것을 말합니다. 가상 / 증강 현실, 메타버스에 대한 관심이 높아짐에 따라 view synthesis에 대한 관심, 연구도 함께 집중되고 있으며, NeRF는 특히 많은 관심을 받아 다양한 후속 연구들이 쏟아져 나오고 있습니다. 

그럼, 지금부터 NeRF란 기술이 생소하시거나 궁금하신 분들을 위해 NeRF가 어떤 것인지, 어떤 과정으로 동작하는지에 대해 소개를 하도록 하겠습니다.

## 1. Differentiable rendering
---
<!-- 기본적으로 **Rendering**이란 형상(geometry), 재질(material), 빛(light), 카메라(camera) 등 scene parameter들에 의해 정의된 3차원 장면에 대한 이미지를 생성하는 프로세스를 말합니다. 반면, **Inverse rendering**은 2D 이미지로부터 scene에 대한 특성, parameter를 예측 / 추론하는 것을 말합니다. -->
**Differentiable rendering**이란, 아래 그림 1과 같이 컴퓨터 그래픽스에서의 rendering 프로세스를 기반으로 scene 파라미터를 통해 생성한 이미지로부터 gradient를 계산하여 최적화하는 프로세스입니다. _(여기서 scene 파라미터란, 형상(geometry), 재질(material), 빛(light), 카메라(camera) 등 장면을 설명할 수 있는 파라미터)_
![Differentiable Renderer 최적화 과정](https://user-images.githubusercontent.com/55485826/178452142-ebccce87-3229-422a-bd76-708c001e32c4.png)
    _그림 1) Differentiable renderer 최적화 과정_

이 프로세스를 통해서 scene 파라미터와 2D 이미지 사이의 관계를 모델링할 수 있고, 다양한 컴퓨터 그래픽스에서의 inverse rendering을 비롯한 다양한 문제들을 해결할 수 있습니다. 특히, 아래 그림 2와 같이 이미지를 neural network의 입력으로 사용해 differentiable rendering을 활용하는 방법은 최근 3차원 관련 분야에서 많이 쓰이고 있습니다.

![이미지 기반 Differentiable Rendering 최적화 과정](https://user-images.githubusercontent.com/55485826/178452202-2cae7e06-248c-412b-9ef0-47567f653efc.png)
    _그림 2) 이미지 기반 Differentiable rendering 최적화 과정_


<!-- - **Why Use Differentiable Rendering?**
    - Inverse rendering 문제를 해결하기 위해 사용
    - Rendering 프로세스를 machine learning 파이프라인에 적용해 문제 해결 -->

## 2. Neural Radiance Fields 란?
---

![Reference: NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis](https://user-images.githubusercontent.com/55485826/178452238-a4eb2b1b-07f2-49d5-ab59-1dfa829be00e.png)
    _그림 3) NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis_

NeRF는 2020년 ECCV에서 Mildenhall 등에 의해 처음 소개 되었습니다. NeRF의 풀네임은 **NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis**로, view synthesis를 위해 scene을 Neural Radiance Field로 표현하는 방법을 제안하였습니다.
NeRF는 일반적인 딥러닝에의 training 방법과 달리 대량의 데이터셋을 사용하지 않고, 하나의 scene에 대해 다양한 시점에서 촬영한 이미지 셋과 카메라 파라미터를 가지고 학습하여 한 scene에 대해 모델링하게 됩니다. 또한, 이미지를 사용하지만 이미지 자체가 딥러닝 네트워크의 입력으로 들어가지 않고, 좌표를 기반으로 하여 계산된 **ray**를 입력으로 사용하게 됩니다. 따라서 학습된 NeRF 네트워크는 이미지 좌표와 카메라 파라미터를 통해 계산한 ray에 대한 color와, density를 예측하고 volume rendering을 통해 새로운 시점에서의 이미지를 rendering 할 수 있게 됩니다.

지금부터, 입력이 어떻게 들어가고 NeRF를 어떻게 학습하는지 간단하게 설명드리도록 하겠습니다.

### Ray 생성
---
앞서 말씀드린 것처럼, 이미지 좌표와 카메라 파라미터를 사용해 ray를 생성하여 학습을 위한 입력으로 사용합니다. Ray란, 카메라에서 3차원 공간 상의 한 물체의 한 점까지 날아갈 때 생기는 직선을 말합니다. Ray는 카메라의 원점(origin)과 방향(viewing direction)이 정해지면 다음과 같이 표현할 수 있습니다.
<img src="https://user-images.githubusercontent.com/55485826/179489036-345a0fb8-ff49-4486-90b1-f57bda693510.png" width="200" height="400"/>
    _여기서 t는 카메라로부터 물체의 한 점까지의 거리를 의미합니다._



## 3. NeRF의 한계
---

## 4. 앞으로의 방향성
---
